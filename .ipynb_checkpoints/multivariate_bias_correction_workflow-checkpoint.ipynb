{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8604670b-e13a-4a26-97fe-beac75b55dd0",
   "metadata": {},
   "source": [
    "# Multivariate bias adjustment workflow\n",
    "\n",
    "This notebook provides an example workflow for multivariate bias adjustment (MBA) in the context of \n",
    "thermal stress. The workflow uses four input variables (air temperature, relative humidity, wind speed, \n",
    "and downwelling solar radiation) to calculate mean radiant temperature (Tmrt), and the Universal Thermal \n",
    "Climate Index (UTCI). We use the BARRA reanalysis (Su et al., 2021) as our historical observations, and the CSIRO's ACCCESS1-0 model, dynamically downscaled by Harris et al. (2020) and Clarke et al. (2019). The workflow makes significant use of the compatability of the xclim and xarray packages.\n",
    "\n",
    "To achieve this, we emply a diurnal version of Cannon's N-dimensional multivariate bias correction (MBCn)\n",
    "(2018), guided by the work of Faghih (2022). Each variable undergoes individual bias adjustment in the \n",
    "form of Quantile Delta Mapping (QDM) (Cannon et al., 2015). These adjustments are multiplicative for all\n",
    "variables. To retain diurnal and annual cycles, inputs are grouped by hour of the day and week of the year, with weeks 52 and 53 combined to avoid week 53 containing few data points. 30 quantiles are used for each group. Mismatches in solar radiation between our historic model outputs and \"observations\" (reanalysis) very occasionally lead to infinite adjustment factors. Accordingly, we set these adjustment factors to zero. This occurred in 0.065% of cases for the six models in Weeding et al. (2023). \n",
    "\n",
    "Once each input variable is individually adjusted, the variables are adjusted as a group using the \n",
    "MBCn algorithm. Bias adjusted inputs are grouped by hour of the day and day of the year to retain the \n",
    "diurnal and annual cycles as strongly as possible. 20 quantiles were used per group, and 20 iterations \n",
    "performed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1a3b4d6-e23b-4bc3-ae03-11debe8b5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib.colors import LogNorm\n",
    "import sys\n",
    "import xclim\n",
    "from xclim import sdba\n",
    "from xclim.sdba.adjustment import QuantileDeltaMapping\n",
    "import cftime\n",
    "from tabulate import tabulate\n",
    "\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e168b103-2635-45ad-8a5c-09a1735ce9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the historic \"observations\" (in this case BARRA reanalysis),\n",
    "ds_barra = xr.open_zarr('/scratch/bweeding/SOLWEIG_testing/BARRA/bw_alldata_UTCI.zarr',chunks=None)\n",
    "\n",
    "# the historic model outputs,\n",
    "ds_model_hist = xr.open_zarr('/scratch/bweeding/SOLWEIG_testing/CSIRO_BOM_ACCESS1_0/hist/bw_alldata_UTCI.zarr',chunks=None)\n",
    "\n",
    "# and the future model ouputs.\n",
    "ds_model_proj = xr.open_zarr('/scratch/bweeding/SOLWEIG_testing/CSIRO_BOM_ACCESS1_0/proj/bw_alldata_UTCI.zarr',chunks=None)\n",
    "\n",
    "# We trim the observations and historic model outputs to overlap in time exactly\n",
    "start_time = ds_barra.timestamp[0]\n",
    "end_time = ds_model_hist.timestamp[-1]\n",
    "\n",
    "ds_barra = ds_barra.sel(timestamp=slice(start_time,end_time))\n",
    "ds_model_hist = ds_model_hist.sel(timestamp=slice(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb2fc69-d155-425a-a2d5-af5888dbcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make some adjustments to the imported datasets in order for them to work with xclim\n",
    "def set_units(ds):\n",
    "            \n",
    "    ds = ds.rename({'timestamp':'time'})             # the time dimension must be named \"time\"\n",
    "    \n",
    "    ds['Tair'] = ds['Tair'] + 273.15                 # not required if your temperature is already in Kelvin\n",
    "\n",
    "    ds['Tair'] = ds.Tair.assign_attrs({'units':'K'}) # xclim requires the variables to have units\n",
    "\n",
    "    ds['RH'] = ds.RH.assign_attrs({'units':'%'})\n",
    "\n",
    "    ds['Uwind'] = ds.Uwind.assign_attrs({'units':'m/s'})\n",
    "\n",
    "    ds['kdown'] = ds.kdown.assign_attrs({'units':'W m^-2'})\n",
    "\n",
    "    return ds\n",
    "\n",
    "ds_barra = set_units(ds_barra)\n",
    "ds_model_hist = set_units(ds_model_hist)\n",
    "ds_model_proj = set_units(ds_model_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4c40b-7c27-4cc7-8ce8-3104960980a8",
   "metadata": {},
   "source": [
    "We begin the individual bias adjustments, starting with air temperature (Tair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bed7c53-eb6d-4e07-af4d-5297194b7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make copies of the model outputs that will be altered by the bias adjusting\n",
    "Tair_hist_adjusted_QDM =  ds_model_hist.Tair.copy()\n",
    "\n",
    "Tair_proj_adjusted_QDM = ds_model_proj.Tair.copy()\n",
    "\n",
    "# We group the outputs by hour of the day and...\n",
    "for cur_hour in np.arange(0,24,1):\n",
    "    \n",
    "    # week of the year. We don't include week 53 in this index as it is captured with week 52.\n",
    "    for cur_week in np.arange(1,53,1):\n",
    "        \n",
    "        # Here we combine outputs for weeks 52 and 53\n",
    "        if cur_week == 52:\n",
    "            \n",
    "            # For each combination of hour of the day and week of the year we train a Quantile Delta Mapping\n",
    "            # (QDM) object on the group of outputs, with 30 quantiles (nquantiles=30) and multiplicative \n",
    "            # adjustment (kind=\"*\"). As we have already grouped the inputs, we instruct the QDM object not to\n",
    "            # perform any grouping on the time dimension (group=\"time\")\n",
    "            QDM_Tair = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.Tair[(ds_barra.Tair.time.dt.hour==cur_hour)&(np.isin(ds_barra.Tair.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                ds_model_hist.Tair[(ds_model_hist.Tair.time.dt.hour==cur_hour)&(np.isin(ds_model_hist.Tair.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                nquantiles=30, kind=\"*\", group=\"time\")\n",
    "            \n",
    "            # We create time indices for groups of hour of the day and week of the year that combine\n",
    "            # weeks 52 and 53 to insert the adjusted data \n",
    "            hist_time_idx = (Tair_hist_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(Tair_hist_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            proj_time_idx = (Tair_proj_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(Tair_proj_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            # We adjust the model outputs and insert the adjusted data into the copies of the model outputs\n",
    "            Tair_hist_adjusted_QDM[hist_time_idx] = QDM_Tair.adjust(ds_model_hist.Tair[hist_time_idx],interp='nearest')\n",
    "\n",
    "            Tair_proj_adjusted_QDM[proj_time_idx] = QDM_Tair.adjust(ds_model_proj.Tair[proj_time_idx],interp='nearest')\n",
    "\n",
    "        # for all other weeks we perform the same process as above    \n",
    "        else:\n",
    "\n",
    "            # For each combination of hour of the day and week of the year we train a Quantile Delta Mapping\n",
    "            # (QDM) object on the group of outputs, with 30 quantiles (nquantiles=30) and multiplicative \n",
    "            # adjustment (kind=\"*\"). As we have already grouped the inputs, we instruct the QDM object not to\n",
    "            # perform any grouping on the time dimension (group=\"time\")\n",
    "            QDM_Tair = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.Tair[(ds_barra.Tair.time.dt.hour==cur_hour)&(ds_barra.Tair.time.dt.isocalendar().week==cur_week)], \n",
    "                ds_model_hist.Tair[(ds_model_hist.Tair.time.dt.hour==cur_hour)&(ds_model_hist.Tair.time.dt.isocalendar().week==cur_week)], \n",
    "                nquantiles=30, kind=\"*\", group='time')\n",
    "\n",
    "            # We create time indices for groups of hour of the day and week of the year to insert the adjusted data \n",
    "            hist_time_idx = (Tair_hist_adjusted_QDM.time.dt.hour==cur_hour)&(Tair_hist_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            proj_time_idx = (Tair_proj_adjusted_QDM.time.dt.hour==cur_hour)&(Tair_proj_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            # We adjust the model outputs and insert the adjusted data into the copies of the model outputs\n",
    "            Tair_hist_adjusted_QDM[hist_time_idx] = QDM_Tair.adjust(ds_model_hist.Tair[hist_time_idx],interp='nearest')\n",
    "\n",
    "            Tair_proj_adjusted_QDM[proj_time_idx] = QDM_Tair.adjust(ds_model_proj.Tair[proj_time_idx],interp='nearest')\n",
    "\n",
    "# Once adjustments have been made for all combinations of hour of day and week of year, we create new versions\n",
    "# of the adjusted data named in accordance with the xclim template\n",
    "scenh_Tair = Tair_hist_adjusted_QDM\n",
    "scens_Tair = Tair_proj_adjusted_QDM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd448e-d2f6-44fa-a8f3-a558bfb26a46",
   "metadata": {},
   "source": [
    "We now perform the same process for relative humidity and wind speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9360f29-2e41-4c89-a882-b130ca4a8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RH_hist_adjusted_QDM =  ds_model_hist.RH.copy()\n",
    "\n",
    "RH_proj_adjusted_QDM = ds_model_proj.RH.copy()\n",
    "\n",
    "for cur_hour in np.arange(0,24,1):\n",
    "    \n",
    "    for cur_week in np.arange(1,53,1):\n",
    "        \n",
    "        if cur_week == 52:\n",
    "            \n",
    "            QDM_RH = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.RH[(ds_barra.RH.time.dt.hour==cur_hour)&(np.isin(ds_barra.RH.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                ds_model_hist.RH[(ds_model_hist.RH.time.dt.hour==cur_hour)&(np.isin(ds_model_hist.RH.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                nquantiles=30, kind=\"*\", group='time')\n",
    "            \n",
    "            hist_time_idx = (RH_hist_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(RH_hist_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            proj_time_idx = (RH_proj_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(RH_proj_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            RH_hist_adjusted_QDM[hist_time_idx] = QDM_RH.adjust(ds_model_hist.RH[hist_time_idx],interp='nearest')\n",
    "\n",
    "            RH_proj_adjusted_QDM[proj_time_idx] = QDM_RH.adjust(ds_model_proj.RH[proj_time_idx],interp='nearest')\n",
    "\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            QDM_RH = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.RH[(ds_barra.RH.time.dt.hour==cur_hour)&(ds_barra.RH.time.dt.isocalendar().week==cur_week)], \n",
    "                ds_model_hist.RH[(ds_model_hist.RH.time.dt.hour==cur_hour)&(ds_model_hist.RH.time.dt.isocalendar().week==cur_week)], \n",
    "                nquantiles=30, kind=\"*\", group='time')\n",
    "        \n",
    "            hist_time_idx = (RH_hist_adjusted_QDM.time.dt.hour==cur_hour)&(RH_hist_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            proj_time_idx = (RH_proj_adjusted_QDM.time.dt.hour==cur_hour)&(RH_proj_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            RH_hist_adjusted_QDM[hist_time_idx] = QDM_RH.adjust(ds_model_hist.RH[hist_time_idx],interp='nearest')\n",
    "\n",
    "            RH_proj_adjusted_QDM[proj_time_idx] = QDM_RH.adjust(ds_model_proj.RH[proj_time_idx],interp='nearest')\n",
    "\n",
    "scenh_RH = RH_hist_adjusted_QDM\n",
    "scens_RH = RH_proj_adjusted_QDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28c60330-bbbd-453b-b267-7a850ca0c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uwind_hist_adjusted_QDM =  ds_model_hist.Uwind.copy()\n",
    "\n",
    "Uwind_proj_adjusted_QDM = ds_model_proj.Uwind.copy()\n",
    "\n",
    "for cur_hour in np.arange(0,24,1):\n",
    "    \n",
    "    for cur_week in np.arange(1,53,1):\n",
    "        \n",
    "        if cur_week == 52:\n",
    "            \n",
    "            QDM_Uwind = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.Uwind[(ds_barra.Uwind.time.dt.hour==cur_hour)&(np.isin(ds_barra.Uwind.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                ds_model_hist.Uwind[(ds_model_hist.Uwind.time.dt.hour==cur_hour)&(np.isin(ds_model_hist.Uwind.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                nquantiles=30, kind=\"*\", group='time')\n",
    "            \n",
    "            hist_time_idx = (Uwind_hist_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(Uwind_hist_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            proj_time_idx = (Uwind_proj_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(Uwind_proj_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            Uwind_hist_adjusted_QDM[hist_time_idx] = QDM_Uwind.adjust(ds_model_hist.Uwind[hist_time_idx],interp='nearest')\n",
    "\n",
    "            Uwind_proj_adjusted_QDM[proj_time_idx] = QDM_Uwind.adjust(ds_model_proj.Uwind[proj_time_idx],interp='nearest')\n",
    "\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            QDM_Uwind = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.Uwind[(ds_barra.Uwind.time.dt.hour==cur_hour)&(ds_barra.Uwind.time.dt.isocalendar().week==cur_week)], \n",
    "                ds_model_hist.Uwind[(ds_model_hist.Uwind.time.dt.hour==cur_hour)&(ds_model_hist.Uwind.time.dt.isocalendar().week==cur_week)], \n",
    "                nquantiles=30, kind=\"*\", group='time')\n",
    "        \n",
    "            hist_time_idx = (Uwind_hist_adjusted_QDM.time.dt.hour==cur_hour)&(Uwind_hist_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            proj_time_idx = (Uwind_proj_adjusted_QDM.time.dt.hour==cur_hour)&(Uwind_proj_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            Uwind_hist_adjusted_QDM[hist_time_idx] = QDM_Uwind.adjust(ds_model_hist.Uwind[hist_time_idx],interp='nearest')\n",
    "\n",
    "            Uwind_proj_adjusted_QDM[proj_time_idx] = QDM_Uwind.adjust(ds_model_proj.Uwind[proj_time_idx],interp='nearest')\n",
    "\n",
    "scenh_Uwind = Uwind_hist_adjusted_QDM\n",
    "scens_Uwind = Uwind_proj_adjusted_QDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96058a14-2820-45d8-9489-82f8f94dc36f",
   "metadata": {},
   "source": [
    "We now adjust downwelling solar radiation (kdown) using the same process as for the other variables, but with an additional step. Mismatches in solar radiation between our historic model outputs and \"observations\" (reanalysis) very occasionally lead to infinite adjustment factors. Accordingly, we set these adjustment factors to zero. We also set any nan outputs to zero. These occur at night-time hours when both the historic outputs and observations have zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b43246ef-1936-44c2-8091-9fade863c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour:0\n",
      "hour:1\n",
      "hour:2\n",
      "hour:3\n",
      "hour:4\n",
      "hour:5\n",
      "hour:6\n",
      "hour:7\n",
      "hour:8\n",
      "hour:9\n",
      "hour:10\n",
      "hour:11\n",
      "hour:12\n",
      "hour:13\n",
      "hour:14\n",
      "hour:15\n",
      "hour:16\n",
      "hour:17\n",
      "hour:18\n",
      "hour:19\n",
      "hour:20\n",
      "hour:21\n",
      "hour:22\n",
      "hour:23\n"
     ]
    }
   ],
   "source": [
    "kdown_hist_adjusted_QDM =  ds_model_hist.kdown.copy()\n",
    "\n",
    "kdown_proj_adjusted_QDM = ds_model_proj.kdown.copy()\n",
    "\n",
    "num_nans = 0\n",
    "num_infs = 0\n",
    "\n",
    "for cur_hour in np.arange(0,24,1):\n",
    "    \n",
    "    for cur_week in np.arange(1,53,1):\n",
    "        \n",
    "        if cur_week == 52:\n",
    "            \n",
    "            QDM_kdown = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.kdown[(ds_barra.kdown.time.dt.hour==cur_hour)&(np.isin(ds_barra.kdown.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                ds_model_hist.kdown[(ds_model_hist.kdown.time.dt.hour==cur_hour)&(np.isin(ds_model_hist.kdown.time.dt.isocalendar().week,np.array([52,53])))], \n",
    "                nquantiles=30, kind=\"*\", group='time')\n",
    "            \n",
    "            # here we set the infinite and nan adjustment factors to zero\n",
    "            QDM_kdown.ds.af.values = xr.where(QDM_kdown.ds.af==np.inf,0,QDM_kdown.ds.af)\n",
    "            QDM_kdown.ds.af.values = xr.where(np.isnan(QDM_kdown.ds.af),0,QDM_kdown.ds.af)\n",
    "            \n",
    "            hist_time_idx = (kdown_hist_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(kdown_hist_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            proj_time_idx = (kdown_proj_adjusted_QDM.time.dt.hour==cur_hour)&(np.isin(kdown_proj_adjusted_QDM.time.dt.isocalendar().week,np.array([52,53])))\n",
    "\n",
    "            kdown_hist_adjusted_QDM[hist_time_idx] = QDM_kdown.adjust(ds_model_hist.kdown[hist_time_idx],interp='nearest')\n",
    "\n",
    "            kdown_proj_adjusted_QDM[proj_time_idx] = QDM_kdown.adjust(ds_model_proj.kdown[proj_time_idx],interp='nearest')\n",
    "\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            QDM_kdown = sdba.QuantileDeltaMapping.train(\n",
    "                ds_barra.kdown[(ds_barra.kdown.time.dt.hour==cur_hour)&(ds_barra.kdown.time.dt.isocalendar().week==cur_week)], \n",
    "                ds_model_hist.kdown[(ds_model_hist.kdown.time.dt.hour==cur_hour)&(ds_model_hist.kdown.time.dt.isocalendar().week==cur_week)], \n",
    "                nquantiles=30, kind=\"*\", group='time')\n",
    "\n",
    "            # here we set the infinite and nan adjustment factors to zero\n",
    "            QDM_kdown.ds.af.values = xr.where(QDM_kdown.ds.af==np.inf,0,QDM_kdown.ds.af)\n",
    "            QDM_kdown.ds.af.values = xr.where(np.isnan(QDM_kdown.ds.af),0,QDM_kdown.ds.af)\n",
    "        \n",
    "            hist_time_idx = (kdown_hist_adjusted_QDM.time.dt.hour==cur_hour)&(kdown_hist_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            proj_time_idx = (kdown_proj_adjusted_QDM.time.dt.hour==cur_hour)&(kdown_proj_adjusted_QDM.time.dt.isocalendar().week==cur_week)\n",
    "\n",
    "            kdown_hist_adjusted_QDM[hist_time_idx] = QDM_kdown.adjust(ds_model_hist.kdown[hist_time_idx],interp='nearest')\n",
    "\n",
    "            kdown_proj_adjusted_QDM[proj_time_idx] = QDM_kdown.adjust(ds_model_proj.kdown[proj_time_idx],interp='nearest')          \n",
    "            \n",
    "scenh_kdown = kdown_hist_adjusted_QDM\n",
    "scens_kdown = kdown_proj_adjusted_QDM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771baaef-5320-4d0f-b91f-9b60e110b1bc",
   "metadata": {},
   "source": [
    "We make an initial check to ensure maximum and minimum values after adjustment aren't nonsensical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff20c955-9c0b-4219-a306-b4ae313a1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Variable     |     Tair |   kdown |      uwind |        RH |\n",
      "|--------------|----------|---------|------------|-----------|\n",
      "| Historic min | -2.10297 |    0    |  0.0163385 |   9.28976 |\n",
      "| Future min   | -1.56492 |    0    |  0.0332577 |  11.4574  |\n",
      "| Historic max | 44.1873  | 1089.68 | 13.91      | 107.589   |\n",
      "| Future max   | 45.3805  | 1082.93 | 17.7704    | 107.01    |\n"
     ]
    }
   ],
   "source": [
    "table = [['Variable','Tair', 'kdown', 'uwind', 'RH'],\n",
    "         ['Historic min',scenh_Tair.min()-273.15, scenh_kdown.min(), scenh_Uwind.min(), scenh_RH.min()], \n",
    "         ['Future min',scens_Tair.min()-273.15, scens_kdown.min(), scens_Uwind.min(), scens_RH.min()],\n",
    "         ['Historic max',scenh_Tair.max()-273.15, scenh_kdown.max(), scenh_Uwind.max(), scenh_RH.max()], \n",
    "         ['Future max',scens_Tair.max()-273.15, scens_kdown.max(), scens_Uwind.max(), scens_RH.max()],\n",
    "        ]\n",
    "print(tabulate(table, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58c1c7-72c2-4059-abb7-6d961b41092f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f01e35-489e-4e65-a3f8-6246e4105ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ddaf648-25ac-47bb-9ea4-588caf5a733e",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Cannon, A.J., Sobie, S.R. and Murdock, T.Q., 2015. Bias correction of GCM precipitation by quantile mapping: how well do methods preserve changes in quantiles and extremes?. Journal of Climate, 28(17), pp.6938-6959.\n",
    "\n",
    "Cannon, A.J., 2018. Multivariate quantile mapping bias correction: an N-dimensional probability density function transform for climate model simulations of multiple variables. Climate dynamics, 50, pp.31-49.\n",
    "\n",
    "Clarke, J.M., Grose, M., Thatcher, M., Hernaman, V., Heady, C., Round, V., Rafter, T., Trenham, C. and Wilson, L., 2019. Victorian climate projections 2019 technical report. Melbourne Australia.\n",
    "\n",
    "Faghih, M., Brissette, F. and Sabeti, P., 2022. Impact of correcting sub-daily climate model biases for hydrological studies. Hydrology and Earth System Sciences, 26(6), pp.1545-1563.\n",
    "\n",
    "Harris, R; Remenyi, Tomas; Rollins, Dean; Love, Peter; Earl, Nick; Bindoff, Nathaniel (2020). Australia's wine future - climate information  for adaptation to change. University Of Tasmania. Journal contribution. https://hdl.handle.net/102.100.100/23026310.v1\n",
    "\n",
    "Su, C.H., Eizenberg, N., Jakob, D., Fox-Hughes, P., Steinle, P., White, C.J. and Franklin, C., 2021. BARRA v1. 0: kilometre-scale downscaling of an Australian regional atmospheric reanalysis over four midlatitude domains. Geoscientific Model Development, 14(7), pp.4357-4378."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xclim]",
   "language": "python",
   "name": "conda-env-xclim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
